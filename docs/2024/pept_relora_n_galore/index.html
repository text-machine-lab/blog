<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <!--link rel="stylesheet" href="/_sass/jekyll-theme-architect.scss" media="screen" type="text/css">
    <link rel="stylesheet" href="/_sass/normalize.scss" media="screen" type="text/css">
    <link rel="stylesheet" href="/_sass/rouge-github.scss" media="screen" type="text/css"-->
    <link rel="stylesheet" href="/blog/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/blog/assets/css/print.css" media="print" type="text/css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" type="text/css">

    <!--[if lt IE 9]-->
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>

<!-- Default Statcounter code for Text-machine Blog
https://text-machine-lab.github.io/blog/ -->
<script type="text/javascript">
var sc_project=12176921;
var sc_invisible=1;
var sc_security="e9a6e2dd";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12176921/0/e9a6e2dd/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->

<!--Twitter metadata--><meta name="twitter:title" content="Parameter Efficient Pre-Training: Comparing ReLoRA and GaLore">

<!-- end of Twitter metadata -->


    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://text-machine-lab.github.io/blog/assets/images/busters-card.png">
    <meta name="og:image" content="https://text-machine-lab.github.io/blog/assets/images/busters-card.png">




  <meta name="og:description"
    content="Models of BERT family are overall robust to pruning, but they have an Achilles heel: the outlier dimensions, without which the quality of the model drops significantly.">
  <meta name="twitter:description"
    content="Models of BERT family are overall robust to pruning, but they have an Achilles heel: the outlier dimensions, without which the quality of the model drops significantly.">


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Parameter Efficient Pre-Training: Comparing ReLoRA and GaLore | Text Machine Blog</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Parameter Efficient Pre-Training: Comparing ReLoRA and GaLore" />
<meta name="author" content="Namrata Shivagunde" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog discusses two parameter efficient pre-training methods, ReLoRA and GaLore, explaining their core concepts, and key differences." />
<meta property="og:description" content="This blog discusses two parameter efficient pre-training methods, ReLoRA and GaLore, explaining their core concepts, and key differences." />
<link rel="canonical" href="https://text-machine-lab.github.io/blog/2024/pept_relora_n_galore" />
<meta property="og:url" content="https://text-machine-lab.github.io/blog/2024/pept_relora_n_galore/" />
<meta property="og:site_name" content="Text Machine Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-08-31T21:00:00-04:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://text-machine-lab.github.io/blog/2024/pept_relora_n_galore/"},"@type":"BlogPosting","url":"https://text-machine-lab.github.io/blog/2024/pept_relora_n_galore/","headline":"Parameter Efficient Pre-Training: Comparing ReLoRA and GaLore","dateModified":"2021-08-31T21:00:00-04:00","datePublished":"2021-08-31T21:00:00-04:00","author":{"@type":"Person","name":"Anna Rogers"},"description":"This blog discusses two parameter efficient pre-training methods, ReLoRA and GaLore, explaining their core concepts, and key differences.
","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header>
        <div class="nav-menu">
            <ul>
              <li><a href="/blog/tags"><i class="fa fa-hashtag"></i> Tag index</a></li>
              <li><a href="/blog/years"><i class="fa fa-list"></i> All posts</a></li>
              <li><a href="https://github.com/text-machine-lab/"> <i class="fa fa-github"></i> GitHub</a></li>
              <li><a href="http://text-machine.cs.uml.edu/"><i class="fa fa-home"></i> About us</a></li>
            </ul>
        </div>

      <div class="inner">
          <div>
            <a href="http://text-machine.cs.uml.edu/"><img src="/blog/assets/images/text-machine-logo-transparent.png" alt="Text Machine logo" class="logo"></a>
          </div>
        <div>
        <a href="https://text-machine-lab.github.io/blog/">
          <h1>Text Machine Blog</h1>
        </a>
        <h2 class="tagline">Machine Learning, NLP, and more</h2>
        </div>
      </div>

    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          

<h1>Parameter Efficient Pre-Training: Comparing ReLoRA and GaLore</h1>

<span class="post-date">06 May 2024 ‚Ä¢ </span>
<img src="/blog/assets/images/time-button.jpg" class="read-time"></img>
<span class="reading-time" title="Estimated read time">
  
  
    8 mins
  
</span>

<p><strong>Tags:</strong>
  <span>
  
    
    <a href="/tag/transformers"><code class="highligher-rouge"><nobr>transformers</nobr></code>&nbsp;</a>
  
   </span>

</p>



<!--p class="post-author">Author: Anna Rogers</p-->
  
      <hr>
<span>

<div class="author-container">
        
    <div class="author-text">
        <span class="author-name"> Namrata Shivagunde &nbsp;</span>
           
              <a href="https://text-machine.cs.uml.edu/lab2/people/nshivagunde/"><span class="label"><i class="fa fa-home"></i> Profile</span></a>
           
           
              <a href="https://twitter.com/namshivagunde"><span class="label"><i class="fa fa-twitter"></i> Twitter</span></a>
           
           
           <br/> <i class="fa id-badge"></i> <i>Namrata Shivagunde is a PhD student at Text Machine Lab, University of Massachusetts Lowell.</i>
           
    </div>
</div>


  

<h2 id="intro">Parameter Efficient Pre-training (PEPT)</h2>

	<p>Large language models (LLMs) have revolutionized various natural language processing (NLP) tasks, but their massive parameter count creates challenges such as high computational cost and limited accessibility. Parameter-efficient fine-tuning (PEFT) methods have addressed these problems by reducing the resources needed to fine-tune LLMs for specific tasks. This raises the question: can we use parameter-efficient training methods and achieve similar efficiency gains during the pre-training stage too?</p>
	<p>Parameter-efficient pre-training (PEPT) is an emerging area of research that explores techniques for pre-training LLMs with fewer parameters. PEPT has the potential to significantly reduce the computational cost associated with pre-training large language models. Multiple studies suggest that neural network training is either low-rank or has multiple phrases with initially high-rank and subsequent low-rank training (Aghajanyan et al., 2021, Arora et al., 2019, Frankle et al., 2019).</p>  
	<p>ReLoRA (Lialin et. al, 2023)  is the first parameter-efficient training method used to pre-train large language models. ReLoRA uses LoRA decomposition, merges and resets the values of the LoRA matrices multiple times during training, increasing the total rank of the update. Another recent advance in PEPT is GaLore (Zhao et. al, 2024). In GaLore, the gradient is projected into its lower rank form, updated using an optimizer, and projected back to its original shape, reducing the memory requirement for pre-training LLMs by a huge margin.</p>
	<p>This blog discusses ReLoRA and GaLore, explaining their core concepts, and key differences.

<h2 id="relora">ReLoRA: High-Rank Training Through Low-Rank Updates</h2>

	<p>ReLoRA uses LoRA (Hu et al., 2022) decomposition technique where the pre-trained model weights are frozen and trainable rank decomposition matrices (WA, WB) are injected into each layer of the LLM. However in LoRA, the rank of the matrix is restricted by the rank r (given below), and the new trainable parameters (WA and WB) are merged back to the original matrices only after the end of the training. </p>
	 <script type="math/tex">
        	r = \text{rank}(W_{A}W_{B})
    	</script>
	<p>To increase the total rank of the updates, ReLoRA uses the property of the rank of the sum of two matrices:  rank(A + B) ‚â§ rank(A) + rank(B). ReLoRA merges the LoRA matrices with the original matrices multiple times during training leading to the total rank of the update (delW).</p>
	<p>While ReLoRA's low-rank updates and merge-and-reinitialize approach offer efficiency gains and high-rank updates, there are a few challenges. Since the optimizer relies on ADAM, the updates are still highly correlated. ReLoRA performs a partial reset (>90%) of the optimizer state, focusing on pruning magnitudes. This helps break the correlation between updates and ensures the optimization process remains stable. However, this led to an exploding loss. A solution to this problem is to use a jagged learning rate scheduler where on every optimizer reset, the learning rate is set to zero and a quick (50-100 steps) learning rate warm-up is performed to bring it back to the cosine schedule (Fgure 1). This prevents the loss function from diverging significantly after the optimizer reset. Additionally, ReLoRA uses a warm start to gain better performance. In warm start, the model begins with a full-rank training phase for a portion of the training process (typically around 25%) before switching to the low-rank training phase. </p>

	<p>ReLoRA is described in Algorithm 1.</p>
	
<h2 id="galore">GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</h2>
	<p>GaLore is a memory-efficient pre-training technique gradients of the weight matrices are projected into low-rank form, updated using an optimizer, and projected back to the original gradient shape which is used to update the model weights. This technique is based on lemmas and theorems mentioned in the paper. The main lemma and theorem are described below.</p>
	<ul>
		<li><b>Lemma: Gradient becomes low-rank during training</b><be /> 
			If the gradient is of form Gt = A - BWtC, with constant A and PSD matrices B and C, the G converges to rank-1 exponentially, suggesting that the gradient of the given form becomes low rank during training.</li>
		<li><b>Theorem: Gradient Form of reversible models</b><be />
			A reversible network with L2 objective has the gradient of form Gt = A - BWtC. The definition of reversible networks is mentioned in the paper. The paper proves that Feed Forward networks and softmax loss function are reversible networks thus having a gradient of the given form. The paper does not discuss if attention is a reversible network</li>
	</ul>
	<p>As LLMs are made of feed-forward networks and activation functions, based on the above lemma and theorem and its proof, it is implied that LLMs have a gradient of form Gt = A - BWtC and the gradient becomes low rank and training progresses.</p>
	<p>GaLore decomposes the gradient G‚Äô into P, low-rank gradient G, and Q matrices using SVD. P is m x r, Q is r x n projection matrices, where r is the rank. At every step, either P or Q is used depending on if m <= n. G‚Äô is updated using an optimizer (e.g. AdamW). The updated G‚Äô is then projected back to the original space using the transpose of projection matrices P or Q. </p>
	<p>GaLore switches subspaces by reinitializing the projection matrices after a certain number of steps i.e. update frequency. The idea is the model learns in a subspace for a certain number of steps and then switches to another subspace using different initialization of the projection matrices. The projection matrices are re-initialized using the current gradient. Figure 2 shows a geometric interpretation of the low-rank subspace updates.</p>
	<p>The algorithm is given here: </p>

<h2 id="comparison">Comparison between ReLoRA and GaLore</h2>

	<table>
	    <thead>
	        <tr>
	            <th></th>
	            <th>ReLoRA</th>
	            <th>GaLore</th>
	            <th>Decomposition</th>
	            <th>LoRA</th>
	            <th>SVD</th>
	        </tr>
	    </thead>
	    <tbody>
	        <tr>
	            <td>Perplexity difference</td>
	            <td>0.44 (1.3B model)</td>
	            <td>0.08 (1B model)</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Full rank v/s</td>
	            <td></td>
	            <td></td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Tokens trained on</td>
	            <td>23.1B</td>
	            <td>13.1B</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Weight equation</td>
	            <td>W = W + AB</td>
	            <td>W = W + PtGP if m&lt;n</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Gradient form</td>
	            <td>No conditions</td>
	            <td>Gt = A - BWtC, with constant A and PSD matrices B and C, Change of subspace</td>
	            <td>Using Optimizer reset</td>
	            <td>Re-initialization of P</td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Number of matrices trained</td>
	            <td>2, A (m x r) and B (r x n)</td>
	            <td>1, G (m x r) or (r x n)</td>
	            <td>Able to use higher rank as only one matrix is being trained</td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Additional hyperparameters</td>
	            <td>3, optimizer state pruning percentage, reset frequency, rank</td>
	            <td>3, rank, projection rate, update frequency</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Memory required (1B scale)</td>
	            <td>6.17 G</td>
	            <td>4.38 G</td>
	            <td>Throughout</td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Throughput</td>
	            <td>7.4 ex/sec (given 1 RTX 3090, 25G)</td>
	            <td>6.3 ex/sec (1 RTX 4090, 25G)</td>
	            <td>Warmup required</td>
	            <td>Yes</td>
	            <td>No</td>
	        </tr>
	        <tr>
	            <td>Rank (1B scale)</td>
	            <td>128</td>
	            <td>512</td>
	            <td>(at 1024, GaLore is better than full training)</td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Works with</td>
	            <td>-</td>
	            <td>8-bit optimizers, Per-layer weight updates</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	        <tr>
	            <td>Optimizers</td>
	            <td>AdamW</td>
	            <td>8-bit Adam, AdamW, Adafactor</td>
	            <td></td>
	            <td></td>
	            <td></td>
	        </tr>
	    </tbody>
	</table>

	
	<figure>
		<img src="/blog/assets/images/relora-rank-property.png" width="400" height="50" /> 	
	<figcaption>Fig. 1. LORA rank is restricted by rank r. </figcaption>
</figure>

	
<table>
<tbody>
<tr>
<td style="padding:10px;text-align:left">Original paragraph</td>
  <td style="padding:10px;text-align:left">Ghostbusters was [<span style="color:blue;">released</span>] on June 8 , [<span style="color:blue;">1984</span>] , to critical [<span style="color:blue;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:blue;">received</span>] for its deft blend of comedy, [<span style="color:blue;">action</span>] , and horror , and Murray ' s performance was [<span style="color:blue;">repeatedly</span>] singled out for praise .</td>
</tr>
<tr>
<td style="padding:10px;text-align:left">RoBERTa (full model)</td>
<td style="padding:10px;text-align:left">Ghostbusters was [<span style="color:green">released</span>] on June 8 , [<span style="color:#DA8128;">1986</span>] , to critical [<span style="color:green;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:green;">received</span>] for its deft blend of comedy, [<span style="color:green;">action</span>] , and horror , and Murray ' s performance was [<span style="color:#DA8128;">often</span>] singled out for praise .</td>
</tr>
<tr>
<td style="padding:10px;text-align:left">Roberta with outlier dimensions disabled</td>
  <td style="padding:10px;text-align:left"><span style="color:red;">{ lock</span> was [<span style="color:red;">never</span>] on June 8 , [<span style="color:red;">&lt;/s&gt;</span>] , to <span style="color:red;">rely</span> [<span style="color:red;">,</span>] and . It was well [<span style="color:#DA8128;">known</span>] for its <span style="color:red;">acker</span> of comedy , [<span style="color:red;">dinner</span>], and horror , and Murray ' s was [<span style="color:red;">ever</span>] <span style="color:red;">, &lt;/s&gt; &lt;/s&gt; ) </span></td>
</tr>
<tr>
<td style="padding:10px;text-align:left">Roberta with random dimensions disabled</td>
<td style="padding:10px;text-align:left">Ghostbusters was [<span style="color:green">released</span>] on June 8 , [<span style="color:#DA8128;">1986</span>] , to critical [<span style="color:green;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:green;">received</span>] for its deft blend of comedy,  [<span style="color:green;">action</span>] , and horror , and Murray ' s performance was [<span style="color:#DA8128;">particularly</span>] singled out for praise.</td>
</tr>
</tbody>
</table>

<p><br /></p>

<p>The downstream tasks take a hit too. Here is what happens with BERT-base on GLUE when one outlier dimension is disabled at a time:</p>

<figure>
	<img src="/blog/assets/images/outliers-glue.png" /> 	
	<figcaption>Fig. 2. Performance of BERT-base on GLUE benchmark tasks with output LayerNorm dimensions disabled one at a time. X-axis: which dimensions are disabled. Y-axis: performance metric: loss (blue), accuracy (green), correlation coefficients (purple, orange). </figcaption>
</figure>

<h2 id="is-this-a-bug-or-a-feature">Is this a bug or a feature?</h2>

<p>To begin this, the outlier dimensions do not seem to be an artifact of a particular model instance. We found such dimensions in all six models of BERT family that we considered: BERT-small, BERT-medium, BERT-base, BERT-large, mBERT and RoBERTa. They are also found in ELECTRA, XLNet, and BART. A similar phenomenon is present in the output dense layer of GPT-2, (since there the output component is not LayerNorm). It seems that this is a normal effect of pre-training in these models.</p>

<p>To find out when the outliers appear, we pre-train our own  BERT-medium model on BookCorpus <a class="citation" href="#ZhuKirosEtAl_2015_Aligning_Books_and_Movies_Towards_Story-Like_Visual_Explanations_by_Watching_Movies_and_Reading_Books">(Zhu et al., 2015)</a>. We started from a randomly initialized configuration with 8 layers and the hidden dimensionality of 512 units. We saved checkpoints of the model every 2000 steps, and we tracked the output LayerNorm weights across all of the model‚Äôs layers as the training progressed. Figure 3 shows that both scaling factors and biases begin to diverge from their initialization values quite early (after approximately 50k steps) in the training process. At roughly the same point, both training loss and evaluation perplexity begin to improve, which is in line with the drastic effect on model performance that we saw in the above pruning experiments.</p>

<figure>
	<img src="/blog/assets/images/outliers-pretraining.png" /> 	
	<figcaption>Fig. 3. BERT-medium pre-training on the BookCorpus dataset. (left) Evaluation perplexity (brown) and train loss (blue) as the training progresses. (middle) The changes in the scaling factors and the biases of the output normalization layer. Each line corresponds to one of the 512 dimensions. We highlight (in orange) the 417-th dimension, for which both the scaling factor and the bias fall out of the three sigma range at the end of pretraining. (right) Token embeddings computed for an input sequence that was randomly sampled from the data. Each line corresponds to one input token. The outlier embedding values are marked at the same 417-th dimension. All the plots are presented for the middle Transformer layer (4). </figcaption>
</figure>

<h2 id="how-is-this-related-to-positional-embeddings">How is this related to positional embeddings?</h2>

<p>In a concurrent work, <a class="citation" href="#luo-etal-2021-positional">(Luo, Kulmizev, &amp; Mao, 2021)</a> hypothesized that the outlier phenomenon is attributable to the positional embedding layer. We argue <a class="citation" href="#kovaleva2021transformer">(Kovaleva, 2021)</a> it is much more likely that this effect comes from the LayerNorm component of the embedding layer.</p>

<p>The histogram in Fig 4. shows that it‚Äôs only the LayerNorm of the embedding layer that has unusual outlier values in the specified dimension. The distribution of weights in the other three components of the embedding layer ‚Äì lexical, positional, and segment embeddings ‚Äì is centered around zero and forms Gaussian-shaped curves with a small standard deviation.
Compared to the other components, LayerNorm weights have a much higher variance of values, with the highest weight matching the outlier position <tt>308</tt>. We hypothesize that the learned weights of LayerNorm in the embedding layer are responsible for producing high-magnitude outlier features that are propagated through the rest of the network resulting in the consistent outlier effects across the Transformer layers.</p>

<figure>
	<img src="/blog/assets/images/embedding_weights_marked.png" /> 	
<figcaption>Fig. 4. The normalized histograms of weight values extracted from different components of the embedding layer of BERT base. The blue histograms represent the entire set of extracted values within a given component, whereas the red ones refer to the weights that result in the outlier feature <tt>308</tt>. Most of the outlier-corresponding weights are centered around zero, while the LayerNorm weights fall out of the 3-ùúé interval.
</figcaption>
</figure>

<h2 id="where-do-we-go-from-here">Where do we go from here?</h2>

<p>So‚Ä¶ if BERT can be completely disrupted by disabling so few weights, how come this phenomenon hasn‚Äôt been seen in all the pruning studies so far? The catch is that it is not just about the weight magnitude. The outlier dimensions have to be pruned in the exact same position across the model, and neither magnitude pruning nor pruning attention heads based on their importance scores have that constraint. So, simply by not pruning the same positions across the model, the pruning studies have been avoiding the outlier iceberg. Just to avoid degenerate runs by chance, we would recommend that work on pruning/compressing Transformers explicitly puts in the constraint that the pruned weights should not be in the same position across the model.</p>

<p>Now that we know about the outliers, they are an obvious security risk: if a malicious actor gains access to the weights of a model, they could easily identify such weights and modify them (e.g. in a federated learning context). It would look like the model is still running as expected, except that its output would turn into garbage.</p>

<p>And, since the outlier dimensions seem to be a regular feature of Transformer-based models, this brings up a host of interesting questions for future work:</p>

<ul>
  <li>Is it possible to pre-train a Transformer that wouldn‚Äôt have such outliers?</li>
  <li>If they are necessary, shall we save ourselves trouble and initialize the model that way?</li>
  <li>Can their emergence in pre-training be used as an auxiliary signal for when the model training is completed?</li>
  <li>Is 2-3 outlier features necessary sufficient, or will the model quality be improved by creating more of them?</li>
</ul>



<!-- AddToAny BEGIN -->
<script async src="https://static.addtoany.com/menu/page.js"></script>
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<a class="a2a_button_twitter"></a>
<a class="a2a_button_reddit"></a>
<a class="a2a_button_facebook"></a>
<a class="a2a_button_telegram"></a>
<a class="a2a_button_hacker_news"></a>
<a class="a2a_button_email"></a>
<a class="a2a_dd" href="https://www.addtoany.com/share"></a>

<!-- LikeBtn.com BEGIN -->

<span class="likebtn-wrapper" data-theme="custom" data-btn_size="40" data-f_size="14" data-icon_size="30" data-icon_l_c="#159031" data-icon_l_c_v="#1405fb" data-icon_d_c="#f40d20" data-icon_d_c_v="#1405fb" data-identifier="item_1" data-show_like_label="false" data-counter_frmt="km"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->

</div>

<script src="https://utteranc.es/client.js"
        repo="text-machine-lab/blog"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



<h2 id="refs"> References </h2>

<ol class="bibliography"><li><div class="text-justify">
    <span id="ZhuKirosEtAl_2015_Aligning_Books_and_Movies_Towards_Story-Like_Visual_Explanations_by_Watching_Movies_and_Reading_Books">Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., &amp; Fidler, S. (2015). Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 19‚Äì27.</span>

    
    
    
    <a href="https://openaccess.thecvf.com/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="RogersKovalevaEtAl_2020_Primer_in_BERTology_What_We_Know_About_How_BERT_Works">Rogers, A., Kovaleva, O., &amp; Rumshisky, A. (2020). A Primer in BERTology: What We Know About How BERT Works. <i>Transactions of the Association for Computational Linguistics</i>, <i>8</i>, 842‚Äì866. https://doi.org/10.1162/tacl_a_00349</span>

    
    
    
    <a href="https://www.aclweb.org/anthology/2020.tacl-1.54.pdf">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="PrasannaRogersEtAl_2020_When_BERT_Plays_Lottery_All_Tickets_Are_Winning">Prasanna, S., Rogers, A., &amp; Rumshisky, A. (2020). When BERT Plays the Lottery, All Tickets Are Winning. <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 3208‚Äì3229. Online: Association for Computational Linguistics.</span>

    
    
    
    <a href="https://www.aclweb.org/anthology/2020.emnlp-main.259/">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="luo-etal-2021-positional">Luo, Z., Kulmizev, A., &amp; Mao, X. (2021). Positional Artefacts Propagate Through Masked Language Model Embeddings. <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</i>, 5312‚Äì5327. https://doi.org/10.18653/v1/2021.acl-long.413</span>

    
    
    
    <a href="https://aclanthology.org/2021.acl-long.413">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="kovaleva-etal-2021-bert">Kovaleva, O., Kulshreshtha, S., Rogers, A., &amp; Rumshisky, A. (2021). BERT Busters: Outlier Dimensions that Disrupt Transformers. <i>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</i>, 3392‚Äì3405. https://doi.org/10.18653/v1/2021.findings-acl.300</span>

    
    
    
    <a href="https://aclanthology.org/2021.findings-acl.300">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="kovaleva2021transformer">Kovaleva, O. (2021). <i>Transformer Models in Natural Language Understanding: Strengths, Weaknesses, and Limitations</i> (PhD thesis). University of Massachusetts Lowell.</span>

    
    
    
</div>
</li>
<li><div class="text-justify">
    <span id="GaneshChenEtAl_2020_Compressing_Large-Scale_Transformer-Based_Models_Case_Study_on_BERT">Ganesh, P., Chen, Y., Lou, X., Khan, M. A., Yang, Y., Chen, D., ‚Ä¶ Nakov, P. (2020). Compressing Large-Scale Transformer-Based Models: A Case Study on BERT. <i>ArXiv:2002.11985 [Cs, Stat]</i>.</span>

    
    
    
    <a href="http://arxiv.org/abs/2002.11985">[PDF]</a>
    
</div>
</li>
<li><div class="text-justify">
    <span id="ChenFrankleEtAl_2020_Lottery_Ticket_Hypothesis_for_Pre-trained_BERT_Networks">Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., &amp; Carbin, M. (2020). The Lottery Ticket Hypothesis for Pre-Trained BERT Networks. <i>ArXiv:2007.12223 [Cs, Stat]</i>.</span>

    
    
    
    <a href="http://arxiv.org/abs/2007.12223">[PDF]</a>
    
</div>
</li></ol>
<!-- AddToAny END -->


        </section>

        <aside id="sidebar">
          
            <ul class="toc">
  <li><a href="#">BERT Busters: Outlier Dimensions that Disrupt Transformers</a>
    <ul>
      <li><a href="#intro-meet-the-bert-busters">Intro: Meet the BERT Busters!</a></li>
      <li><a href="#what-do-the-outlier-dimensions-do">What do the outlier dimensions do?</a></li>
      <li><a href="#is-this-a-bug-or-a-feature">Is this a bug or a feature?</a></li>
      <li><a href="#how-is-this-related-to-positional-embeddings">How is this related to positional embeddings?</a></li>
      <li><a href="#where-do-we-go-from-here">Where do we go from here?</a></li>
      <li><a href="#refs"> References </a></li>
    </ul>
  </li>
</ul>
          
        </aside>


      </div>
    </div>

    
  </body>
</html>
